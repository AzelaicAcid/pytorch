{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Tensor的数据类型和数据类型转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "## 获取张量的数据类型\n",
    "torch.tensor([1.2, 3.4]).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 改变张量的默认数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "torch.tensor([1.2, 3.4]).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "torch.tensor([1.2, 3.4]).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 改变张量的数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "a.long()方法： torch.int64\n",
      "a.int()方法： torch.int32\n",
      "a.float()方法： torch.float32\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1.2, 3.4])\n",
    "print(a.dtype)\n",
    "print('a.long()方法：', a.long().dtype)\n",
    "print('a.int()方法：', a.int().dtype)\n",
    "print('a.float()方法：', a.float().dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 获取张量默认数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.get_default_dtype()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 张量的生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#通过torch.tensor()构造张量, dtype=...指定数据类型，requires_grad=...指定是否需要计算梯度\n",
    "A = torch.tensor([[1.0, 1.0], [2,2]], dtype=torch.float32, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5.1 张量的属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A.shape: torch.Size([2, 2])\n",
      "A.size(): torch.Size([2, 2])\n",
      "A.numel(): 4\n"
     ]
    }
   ],
   "source": [
    "print('A.shape:', A.shape) #张量的形状\n",
    "print('A.size():', A.size())\n",
    "print('A.numel():', A.numel()) #张量的元素个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2.],\n",
      "        [4., 4.]])\n"
     ]
    }
   ],
   "source": [
    "y = A.pow(2).sum() \n",
    "y.backward()\n",
    "print(A.grad) #张量每个元素的梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.2 ones, zeros,eyes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.ones(2,3))\n",
    "print(torch.zeros(2,3))\n",
    "print(torch.eye(3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.3**_like()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1]])\n",
      "tensor([[0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0]])\n",
      "tensor([[0.4265, 0.4958, 0.8463, 0.6671, 0.4801],\n",
      "        [0.6904, 0.9355, 0.6260, 0.3534, 0.6638]])\n",
      "tensor([[1, 0, 0, 0, 0],\n",
      "        [0, 2, 0, 0, 0],\n",
      "        [0, 0, 3, 0, 0],\n",
      "        [0, 0, 0, 4, 0],\n",
      "        [0, 0, 0, 0, 5]])\n"
     ]
    }
   ],
   "source": [
    "B = torch.arange(0,10).reshape(2,5)\n",
    "print(torch.ones_like(B))\n",
    "print(torch.zeros_like(B))\n",
    "print(torch.rand_like(B.float()))\n",
    "print(torch.diag(torch.tensor((1,2,3,4,5)))) #输出对角张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.4 torch.new_**() 创建类型相同但是尺寸不同的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1]])\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n",
      "tensor([[4692750812804284416, 4728779609836879872, 4665729215021187072],\n",
      "        [4719772410577944576, 4611686019528916992, 4701758012063219712],\n",
      "        [4733283209465823232, 4683743613543251968, 4724276010207412224]])\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "X = torch.diag(torch.tensor((1,2,3,4,5)))\n",
    "print(X.new_full((3,3), fill_value=1))\n",
    "print(X.new_zeros((3,3)))\n",
    "print(X.new_empty((3,3)))\n",
    "print(X.new_ones((3,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.5 张量与Numpy数组相互转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64) tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64) [[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "F = np.ones((3,3))\n",
    "#numpy数组转换为张量\n",
    "Ftensor1 = torch.as_tensor(F)\n",
    "Ftensor2 = torch.from_numpy(F)\n",
    "#张量转换为numpy数组\n",
    "a = Ftensor1.numpy()\n",
    "\n",
    "print(Ftensor1, Ftensor2, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.6 随机数生成张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8885, 2.2407, 1.8911, 3.0383])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123) #手动指定随机数种子\n",
    "#分别产生服从 N(1,1^2),N(2, 2^2),N(3,3^2),N(4,4^2）的四个随机数\n",
    "A_normal = torch.normal(mean=torch.arange(1,5.0), std=torch.arange(1,5.0))\n",
    "print(A_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1841, 0.7264, 0.3153, 0.6871],\n",
      "        [0.0756, 0.1966, 0.3164, 0.4017],\n",
      "        [0.1186, 0.8274, 0.3821, 0.6605]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.rand(3,4)) #服从U(0,1)的3*4张量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8536, 0.5932, 0.6367, 0.9826])\n",
      "tensor([[ 0.2350,  0.6653,  0.3528,  0.9728],\n",
      "        [-0.0386, -0.8861, -0.4709, -0.4269],\n",
      "        [-0.0283,  1.4220, -0.3886, -0.8903]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.rand_like(A_normal)) #服从标准正态分布\n",
    "print(torch.randn(3,4)) #标准正态分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 7, 5, 8, 3, 1, 6, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "print(torch.randperm(9)) #将0-8的整数随机打乱成张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.7 其他生成张量的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 2, 4, 6, 8])\n",
      "tensor([ 0.0000,  2.5000,  5.0000,  7.5000, 10.0000])\n"
     ]
    }
   ],
   "source": [
    "print(torch.arange(start=0, end=10, step=2)) #固定步长\n",
    "print(torch.linspace(0,10,5)) #固定数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 张量的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.arange(12.0).reshape(3,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.1获取张量的元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: tensor([[[ 0.,  1.,  2.,  3.],\n",
      "         [ 4.,  5.,  6.,  7.],\n",
      "         [ 8.,  9., 10., 11.]]])\n",
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n",
      "tensor([[0., 1., 2., 3.],\n",
      "        [4., 5., 6., 7.]])\n"
     ]
    }
   ],
   "source": [
    "#索引和切片\n",
    "A = torch.arange(12.0).reshape(1, 3, 4)\n",
    "print('A:', A)\n",
    "print(A[0]) #获取第0维度下所有元素\n",
    "print(A[0, 0:2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0., -1., -2., -3.],\n",
      "         [-4., -5.,  6.,  7.],\n",
      "         [ 8.,  9., 10., 11.]]])\n",
      "tensor([ 6.,  7.,  8.,  9., 10., 11.])\n"
     ]
    }
   ],
   "source": [
    "##  根据条件筛选\n",
    "B = -A\n",
    "print(torch.where(A>5,A,B)) #A中元素大于5是返回A的元素 ，否则返回B的元素\n",
    "print(A[A>5]) #获取A中大于5的元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  0.,  0.,  0.,  0.],\n",
      "        [ 5.,  6.,  0.,  0.,  0.],\n",
      "        [10., 11., 12.,  0.,  0.],\n",
      "        [15., 16., 17., 18.,  0.],\n",
      "        [20., 21., 22., 23., 24.]])\n",
      "tensor([[ 0.,  0.,  0.,  0.,  0.],\n",
      "        [ 5.,  0.,  0.,  0.,  0.],\n",
      "        [10., 11.,  0.,  0.,  0.],\n",
      "        [15., 16., 17.,  0.,  0.],\n",
      "        [20., 21., 22., 23.,  0.]])\n",
      "tensor([[ 0.,  0.,  0.,  0.,  0.],\n",
      "        [ 5.,  6.,  0.,  0.,  0.],\n",
      "        [10., 11., 12.,  0.,  0.],\n",
      "        [15., 16., 17., 18.,  0.],\n",
      "        [20., 21., 22., 23., 24.]])\n",
      "tensor([[ 0.,  1.,  0.,  0.,  0.],\n",
      "        [ 5.,  6.,  7.,  0.,  0.],\n",
      "        [10., 11., 12., 13.,  0.],\n",
      "        [15., 16., 17., 18., 19.],\n",
      "        [20., 21., 22., 23., 24.]])\n",
      "tensor([[ 0.,  1.,  2.,  3.,  4.],\n",
      "        [ 0.,  6.,  7.,  8.,  9.],\n",
      "        [ 0.,  0., 12., 13., 14.],\n",
      "        [ 0.,  0.,  0., 18., 19.],\n",
      "        [ 0.,  0.,  0.,  0., 24.]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.arange(25.0).reshape(5,5)\n",
    "print(A.tril()) #获取下三角的元素，可以通过diagonal控制对角线\n",
    "print(A.tril(diagonal=-1)) #获取下三角的元素，可以通过diagonal控制对角线\n",
    "print(A.tril(diagonal=0)) #获取下三角的元素，可以通过diagonal控制对角线\n",
    "print(A.tril(diagonal=1)) #获取下三角的元素，可以通过diagonal控制对角线\n",
    "print(A.triu()) #获取上三角的元素\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 0],\n",
      "        [0, 2, 0],\n",
      "        [0, 0, 3]])\n"
     ]
    }
   ],
   "source": [
    "#提供对角线元素生成矩阵\n",
    "print(torch.diag(torch.tensor([1,2,3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.],\n",
      "        [ 3.,  4.,  5.],\n",
      "        [ 0.,  2.,  4.],\n",
      "        [ 6.,  8., 10.]]) torch.Size([4, 3])\n",
      "tensor([[ 0.,  1.,  2.,  0.,  2.,  4.],\n",
      "        [ 3.,  4.,  5.,  6.,  8., 10.]]) torch.Size([2, 6])\n"
     ]
    }
   ],
   "source": [
    "#链接张量\n",
    "A = torch.arange(6.0).reshape(2,3)\n",
    "B = torch.linspace(0,10,6).reshape(2,3)\n",
    "print(torch.cat((A,B),dim=0), torch.cat((A,B),dim=0).shape)#维度0是列\n",
    "print(torch.cat((A,B),dim=1), torch.cat((A,B),dim=1).shape)#维度1是行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.,  1.,  2.],\n",
      "         [ 3.,  4.,  5.]],\n",
      "\n",
      "        [[ 0.,  2.,  4.],\n",
      "         [ 6.,  8., 10.]]]) torch.Size([2, 2, 3])\n",
      "tensor([[[ 0.,  0.],\n",
      "         [ 1.,  2.],\n",
      "         [ 2.,  4.]],\n",
      "\n",
      "        [[ 3.,  6.],\n",
      "         [ 4.,  8.],\n",
      "         [ 5., 10.]]]) torch.Size([2, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "#stack是直接堆叠一起\n",
    "F = torch.stack((A,B), dim=0)\n",
    "print(F, F.shape)\n",
    "F = torch.stack((A,B), dim=2)\n",
    "print(F, F.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.split() 和 torch.chunk()可以对张量分块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.2 张量计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False,  True, False,  True],\n",
      "        [False, False,  True, False],\n",
      "        [ True,  True, False, False]]) tensor([[ True, False,  True, False],\n",
      "        [ True,  True, False,  True],\n",
      "        [False, False,  True,  True]]) tensor([[False, False, False, False],\n",
      "        [False, False, False, False],\n",
      "        [False, False, False, False]])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#比较\n",
    "A = torch.randn((3,4))\n",
    "B = torch.randn((3,4))\n",
    "print(A>B, A<B, A==B)\n",
    "print(torch.equal(A,B))#返回bool值，而不是tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.8118,  1.2623, -0.8829,  0.7073],\n",
      "        [-1.0125, -2.0082,  0.9934, -0.3483],\n",
      "        [ 0.1629,  1.6595, -1.9623, -1.5737]])\n",
      "tensor([[ -0.6735,   0.0880,  -1.1891,   0.4221],\n",
      "        [ -0.4428,   0.0929, -34.9510,   0.6695],\n",
      "        [  1.2878,  -3.3446,  -1.2280,  -4.8070]])\n",
      "tensor([[-0.7894,  0.1687, -0.1934,  0.6323],\n",
      "        [-0.2181,  0.4554, -0.0267,  0.7439],\n",
      "        [ 0.4127, -0.4880, -0.9526, -0.3531]])\n",
      "tensor([[ 0.3535, -1.5060, -0.0763, -1.7405],\n",
      "        [ 0.3910,  2.4196,  0.9381,  1.7598],\n",
      "        [ 1.2951,  0.8956, -0.2008, -1.0317]])\n",
      "tensor([[   nan,    nan,    nan,    nan],\n",
      "        [   nan, 0.0302, 1.0010, 0.6926],\n",
      "        [0.8362, 0.9107,    nan,    nan]])\n",
      "tensor([[    nan,     nan,     nan,     nan],\n",
      "        [    nan, -1.5813, -0.0349, -0.3485],\n",
      "        [-0.3161,  0.2449,     nan,     nan]])\n"
     ]
    }
   ],
   "source": [
    "#逐元素计算\n",
    "print(A-B)\n",
    "print(A/B)\n",
    "print(A*B)\n",
    "print(A+B)\n",
    "print(A**B)\n",
    "print(torch.log(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.,  9.],\n",
      "        [10., 11., 12., 12., 12.],\n",
      "        [12., 12., 12., 12., 12.],\n",
      "        [12., 12., 12., 12., 12.]])\n",
      "tensor([[12., 12., 12., 12., 12.],\n",
      "        [12., 12., 12., 12., 12.],\n",
      "        [12., 12., 12., 13., 14.],\n",
      "        [15., 16., 17., 18., 19.],\n",
      "        [20., 21., 22., 23., 24.]])\n",
      "tensor([[ 8.,  8.,  8.,  8.,  8.],\n",
      "        [ 8.,  8.,  8.,  8.,  9.],\n",
      "        [10., 11., 12., 13., 14.],\n",
      "        [15., 16., 16., 16., 16.],\n",
      "        [16., 16., 16., 16., 16.]])\n"
     ]
    }
   ],
   "source": [
    "#数据裁剪\n",
    "A = torch.arange(25.0).reshape(5,5)\n",
    "print(torch.clamp_max(A, 12)) #最大值裁剪\n",
    "print(torch.clamp_min(A, 12)) #最小值裁剪\n",
    "print(torch.clamp(A, 8, 16)) #范围值裁剪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  5., 10., 15., 20.],\n",
      "        [ 1.,  6., 11., 16., 21.],\n",
      "        [ 2.,  7., 12., 17., 22.],\n",
      "        [ 3.,  8., 13., 18., 23.],\n",
      "        [ 4.,  9., 14., 19., 24.]])\n",
      "tensor([[ 625.0000,  666.6667,  708.3334,  750.0000,  791.6667],\n",
      "        [1666.6666, 1812.5000, 1958.3334, 2104.1667, 2250.0000],\n",
      "        [2708.3333, 2958.3335, 3208.3333, 3458.3333, 3708.3335],\n",
      "        [3750.0000, 4104.1670, 4458.3335, 4812.5000, 5166.6670],\n",
      "        [4791.6665, 5250.0000, 5708.3335, 6166.6670, 6625.0000]])\n",
      "tensor([[ 30.],\n",
      "        [ 80.],\n",
      "        [130.],\n",
      "        [180.],\n",
      "        [230.]])\n"
     ]
    }
   ],
   "source": [
    "## 矩阵计算\n",
    "print(A.T) #转置\n",
    "B = torch.linspace(0, 100, 25).reshape(5,5)\n",
    "print(torch.matmul(A,B)) #矩阵乘法\n",
    "C = torch.arange(5.0).reshape(5,1)\n",
    "print(A.matmul(C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000e+00, 0.0000e+00, 1.1921e-07],\n",
      "        [0.0000e+00, 1.0000e+00, 5.9605e-08],\n",
      "        [1.4901e-08, 5.9605e-08, 1.0000e+00]])\n",
      "tensor(2.0181)\n"
     ]
    }
   ],
   "source": [
    "#矩阵的逆\n",
    "X = torch.rand((3,3))\n",
    "Y = torch.inverse(X) #逆矩阵\n",
    "print(X.matmul(Y))\n",
    "print(X.trace()) #矩阵的迹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-155.9073, -162.9471, -251.2059,  113.6582,  -73.3023],\n",
      "        [ 133.8239,   27.0558,   50.7093,    3.6844,  144.6039],\n",
      "        [-245.9942,   52.7654, -120.5019,  -12.8385, -190.1996],\n",
      "        [ 130.4192,   40.5925,  201.2318, -232.5089, -157.7215],\n",
      "        [ 134.8858,   12.2870,  132.0990,   26.7640,  -49.2329]])\n",
      "最大值： tensor(201.2318)\n",
      "最大值位置： tensor(17)\n",
      "最小值： tensor(-251.2059)\n",
      "最小值位置： tensor(2)\n",
      "每行最大值： torch.return_types.max(\n",
      "values=tensor([113.6582, 144.6039,  52.7654, 201.2318, 134.8858]),\n",
      "indices=tensor([3, 4, 1, 2, 0]))\n",
      "每行最大值位置： tensor([3, 4, 1, 2, 0])\n",
      "tensor([113.6582, 144.6039,  52.7654, 201.2318, 134.8858])\n"
     ]
    }
   ],
   "source": [
    "#统计计算\n",
    "A = torch.randn((5,5)) * 100\n",
    "print(A)\n",
    "print('最大值：', A.max())\n",
    "print('最大值位置：', A.argmax()) #行号为 A.argmax() / 列数， 列号为A.argmax() % 列数\n",
    "print('最小值：', A.min())\n",
    "print('最小值位置：', A.argmin()) #行号为 A.argmax() / 列数， 列号为A.argmax() % 列数\n",
    "print('每行最大值：', A.max(dim=1)) #返回最大值和最大值索引\n",
    "print('每行最大值位置：', A.argmax(dim=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.sort(\n",
      "values=tensor([[-251.2059, -162.9471, -155.9073,  -73.3023,  113.6582],\n",
      "        [   3.6844,   27.0558,   50.7093,  133.8239,  144.6039],\n",
      "        [-245.9942, -190.1996, -120.5019,  -12.8385,   52.7654],\n",
      "        [-232.5089, -157.7215,   40.5925,  130.4192,  201.2318],\n",
      "        [ -49.2329,   12.2870,   26.7640,  132.0990,  134.8858]]),\n",
      "indices=tensor([[2, 1, 0, 4, 3],\n",
      "        [3, 1, 2, 0, 4],\n",
      "        [0, 4, 2, 3, 1],\n",
      "        [3, 4, 1, 0, 2],\n",
      "        [4, 1, 3, 2, 0]]))\n",
      "torch.return_types.sort(\n",
      "values=tensor([[ 113.6582,  -73.3023, -155.9073, -162.9471, -251.2059],\n",
      "        [ 144.6039,  133.8239,   50.7093,   27.0558,    3.6844],\n",
      "        [  52.7654,  -12.8385, -120.5019, -190.1996, -245.9942],\n",
      "        [ 201.2318,  130.4192,   40.5925, -157.7215, -232.5089],\n",
      "        [ 134.8858,  132.0990,   26.7640,   12.2870,  -49.2329]]),\n",
      "indices=tensor([[3, 4, 0, 1, 2],\n",
      "        [4, 0, 2, 1, 3],\n",
      "        [1, 3, 2, 4, 0],\n",
      "        [2, 0, 1, 4, 3],\n",
      "        [0, 2, 3, 1, 4]]))\n"
     ]
    }
   ],
   "source": [
    "#张量排序\n",
    "print(torch.sort(A)) #升序，两个返回值\n",
    "print(torch.sort(A, descending=True)) #降序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-155.9073, -162.9471, -251.2059,  113.6582,  -73.3023],\n",
      "        [ 133.8239,   27.0558,   50.7093,    3.6844,  144.6039],\n",
      "        [-245.9942,   52.7654, -120.5019,  -12.8385, -190.1996],\n",
      "        [ 130.4192,   40.5925,  201.2318, -232.5089, -157.7215],\n",
      "        [ 134.8858,   12.2870,  132.0990,   26.7640,  -49.2329]])\n",
      "torch.return_types.topk(\n",
      "values=tensor([[ 134.8858,   52.7654,  201.2318,  113.6582,  144.6039],\n",
      "        [ 133.8239,   40.5925,  132.0990,   26.7640,  -49.2329],\n",
      "        [ 130.4192,   27.0558,   50.7093,    3.6844,  -73.3023],\n",
      "        [-155.9073,   12.2870, -120.5019,  -12.8385, -157.7215]]),\n",
      "indices=tensor([[4, 2, 3, 0, 1],\n",
      "        [1, 3, 4, 4, 4],\n",
      "        [3, 1, 1, 1, 0],\n",
      "        [0, 4, 2, 2, 3]]))\n"
     ]
    }
   ],
   "source": [
    "#获取张量中前K大的元素\n",
    "print(A)\n",
    "print(torch.topk(A,4,dim=0)) #前4大，返回两个张量，元素和索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-155.9073, -162.9471, -251.2059,  113.6582,  -73.3023],\n",
      "        [ 133.8239,   27.0558,   50.7093,    3.6844,  144.6039],\n",
      "        [-245.9942,   52.7654, -120.5019,  -12.8385, -190.1996],\n",
      "        [ 130.4192,   40.5925,  201.2318, -232.5089, -157.7215],\n",
      "        [ 134.8858,   12.2870,  132.0990,   26.7640,  -49.2329]])\n",
      "torch.return_types.kthvalue(\n",
      "values=tensor([-162.9471,   27.0558, -190.1996, -157.7215,   12.2870]),\n",
      "indices=tensor([1, 1, 4, 4, 1]))\n"
     ]
    }
   ],
   "source": [
    "# 获取第K小的元素\n",
    "print(A)\n",
    "print(torch.kthvalue(A, 2, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-105.9409,   71.9755, -103.3538,   -3.5974,   51.3606])\n",
      "tensor([-529.7043,  359.8773, -516.7688,  -17.9869,  256.8029])\n",
      "tensor([[-155.9073, -318.8544, -570.0603, -456.4020, -529.7043],\n",
      "        [ 133.8239,  160.8798,  211.5890,  215.2734,  359.8773],\n",
      "        [-245.9942, -193.2289, -313.7307, -326.5692, -516.7688],\n",
      "        [ 130.4192,  171.0117,  372.2435,  139.7346,  -17.9869],\n",
      "        [ 134.8858,  147.1728,  279.2718,  306.0359,  256.8029]])\n",
      "torch.return_types.median(\n",
      "values=tensor([-155.9073,   50.7093, -120.5019,   40.5925,   26.7640]),\n",
      "indices=tensor([0, 2, 2, 1, 3]))\n",
      "tensor([[-1.5591e+02,  2.5405e+04, -6.3818e+06, -7.2534e+08,  5.3169e+10],\n",
      "        [ 1.3382e+02,  3.6207e+03,  1.8360e+05,  6.7647e+05,  9.7821e+07],\n",
      "        [-2.4599e+02, -1.2980e+04,  1.5641e+06, -2.0081e+07,  3.8194e+09],\n",
      "        [ 1.3042e+02,  5.2940e+03,  1.0653e+06, -2.4770e+08,  3.9067e+10],\n",
      "        [ 1.3489e+02,  1.6573e+03,  2.1893e+05,  5.8595e+06, -2.8848e+08]])\n",
      "tensor([185.6952,  88.9996, 185.8161, 128.2803, 130.8848])\n"
     ]
    }
   ],
   "source": [
    "print(torch.mean(A, dim=1))  #指定维度计算均值\n",
    "print(torch.sum(A,dim=1)) #指定维度求和\n",
    "print(torch.cumsum(A,dim=1)) #指定维度求累加和\n",
    "print(torch.median(A,dim=1)) #指定维度求中位数，两个返回值\n",
    "print(torch.cumprod(A,dim=1)) #指定维度求累乘积\n",
    "print(torch.std(A,dim=0)) #计算标准差\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
